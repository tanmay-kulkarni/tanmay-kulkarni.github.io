{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"window.location.href = 'about/'; If you are not redirected automatically, click here .","title":"Home"},{"location":"about/","text":"About I am a Data Analytics and Generative AI Engineer with 10 years of experience delivering impactful data solutions across diverse industries. My expertise spans the full data lifecycle\u2014from architecting and building robust data pipelines (batch and streaming) to deploying advanced analytics and AI-driven applications. My core technical skills include: SQL : Advanced data modeling, query optimization, and analytics Python : Data engineering, automation, and machine learning AWS : Cloud-native data architectures and scalable infrastructure dbt : Modern data transformation and analytics engineering Snowflake : Cloud data warehousing and performance tuning Data Pipelines : Design and implementation for both batch and real-time streaming data I am passionate about leveraging data to drive business value, and I thrive in environments that demand both technical depth and creative problem-solving. My experience includes leading end-to-end data projects, collaborating with cross-functional teams, and mentoring engineers in best practices for analytics and AI. Contact You can contact me via: Email: tanmayk1993[@]gmail[dot]com GitHub: GitHub X (Twitter): @DataSuperNerd","title":"About"},{"location":"about/#about","text":"I am a Data Analytics and Generative AI Engineer with 10 years of experience delivering impactful data solutions across diverse industries. My expertise spans the full data lifecycle\u2014from architecting and building robust data pipelines (batch and streaming) to deploying advanced analytics and AI-driven applications. My core technical skills include: SQL : Advanced data modeling, query optimization, and analytics Python : Data engineering, automation, and machine learning AWS : Cloud-native data architectures and scalable infrastructure dbt : Modern data transformation and analytics engineering Snowflake : Cloud data warehousing and performance tuning Data Pipelines : Design and implementation for both batch and real-time streaming data I am passionate about leveraging data to drive business value, and I thrive in environments that demand both technical depth and creative problem-solving. My experience includes leading end-to-end data projects, collaborating with cross-functional teams, and mentoring engineers in best practices for analytics and AI.","title":"About"},{"location":"about/#contact","text":"You can contact me via: Email: tanmayk1993[@]gmail[dot]com GitHub: GitHub X (Twitter): @DataSuperNerd","title":"Contact"},{"location":"certifications/","text":"Certifications Here are some of my certifications: AWS Certified Developer \u2013 Associate AWS Certified Solutions Architect \u2013 Associate SnowPro Core Certification AWS Certified Cloud Practitioner dbt Fundamentals","title":"Certifications"},{"location":"certifications/#certifications","text":"Here are some of my certifications: AWS Certified Developer \u2013 Associate AWS Certified Solutions Architect \u2013 Associate SnowPro Core Certification AWS Certified Cloud Practitioner dbt Fundamentals","title":"Certifications"},{"location":"contact/","text":"","title":"Contact"},{"location":"projects/","text":"Projects Below are some of my GitHub projects. Click on each to preview details in a sidebar: spark-practice-scala another-project","title":"Projects"},{"location":"projects/#projects","text":"Below are some of my GitHub projects. Click on each to preview details in a sidebar: spark-practice-scala another-project","title":"Projects"},{"location":"blog/","text":"Blog Welcome to my blog! Here you'll find posts on data engineering, analytics, AI, and more. Snowflake Tips for Data Engineers Building Data Pipelines on AWS dbt Best Practices for Analytics Engineering Python Data Cleaning Tricks 10 Essential SQL Tips for Data Analysts Sample Post: Data Pipeline Patterns Syntax Highlighting Showcase My Developer Setup","title":"Blog"},{"location":"blog/#blog","text":"Welcome to my blog! Here you'll find posts on data engineering, analytics, AI, and more. Snowflake Tips for Data Engineers Building Data Pipelines on AWS dbt Best Practices for Analytics Engineering Python Data Cleaning Tricks 10 Essential SQL Tips for Data Analysts Sample Post: Data Pipeline Patterns Syntax Highlighting Showcase My Developer Setup","title":"Blog"},{"location":"blog/aws-data-pipelines/","text":"Building Data Pipelines on AWS AWS offers a variety of services for building scalable data pipelines. Here\u2019s a simple example using AWS Glue and S3: Example: Glue ETL Job import sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job sc = SparkContext () glueContext = GlueContext ( sc ) spark = glueContext . spark_session job = Job ( glueContext ) # ... ETL logic ... Orchestrate with Step Functions Use AWS Step Functions to coordinate multiple ETL jobs.","title":"Building Data Pipelines on AWS"},{"location":"blog/aws-data-pipelines/#building-data-pipelines-on-aws","text":"AWS offers a variety of services for building scalable data pipelines. Here\u2019s a simple example using AWS Glue and S3:","title":"Building Data Pipelines on AWS"},{"location":"blog/aws-data-pipelines/#example-glue-etl-job","text":"import sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job sc = SparkContext () glueContext = GlueContext ( sc ) spark = glueContext . spark_session job = Job ( glueContext ) # ... ETL logic ...","title":"Example: Glue ETL Job"},{"location":"blog/aws-data-pipelines/#orchestrate-with-step-functions","text":"Use AWS Step Functions to coordinate multiple ETL jobs.","title":"Orchestrate with Step Functions"},{"location":"blog/data-pipeline-patterns/","text":"Data Pipeline Patterns In this post, I'll discuss some common data pipeline patterns using Python and SQL. Example: Python ETL Script import pandas as pd def extract (): # ... pass def transform ( df ): # ... return df def load ( df ): # ... pass if __name__ == \"__main__\" : df = extract () df = transform ( df ) load ( df ) Example: SQL Window Function SELECT user_id , event_time , ROW_NUMBER () OVER ( PARTITION BY user_id ORDER BY event_time ) AS rn FROM events ; Stay tuned for more posts!","title":"Data Pipeline Patterns"},{"location":"blog/data-pipeline-patterns/#data-pipeline-patterns","text":"In this post, I'll discuss some common data pipeline patterns using Python and SQL.","title":"Data Pipeline Patterns"},{"location":"blog/data-pipeline-patterns/#example-python-etl-script","text":"import pandas as pd def extract (): # ... pass def transform ( df ): # ... return df def load ( df ): # ... pass if __name__ == \"__main__\" : df = extract () df = transform ( df ) load ( df )","title":"Example: Python ETL Script"},{"location":"blog/data-pipeline-patterns/#example-sql-window-function","text":"SELECT user_id , event_time , ROW_NUMBER () OVER ( PARTITION BY user_id ORDER BY event_time ) AS rn FROM events ; Stay tuned for more posts!","title":"Example: SQL Window Function"},{"location":"blog/dbt-best-practices/","text":"dbt Best Practices for Analytics Engineering dbt is a powerful tool for transforming data in the warehouse. Here are some best practices: Use Sources and Seeds Define your raw data as sources and use seeds for static data. Modularize Models Break transformations into small, reusable models. Test Everything -- Example dbt test SELECT * FROM {{ ref ( 'my_model' ) }} WHERE id IS NULL Document Your Models Use dbt's built-in documentation features. Running on cloud Use ECS along with EventBridge to run as an event driven transformation pipeline","title":"dbt Best Practices for Analytics Engineering"},{"location":"blog/dbt-best-practices/#dbt-best-practices-for-analytics-engineering","text":"dbt is a powerful tool for transforming data in the warehouse. Here are some best practices:","title":"dbt Best Practices for Analytics Engineering"},{"location":"blog/dbt-best-practices/#use-sources-and-seeds","text":"Define your raw data as sources and use seeds for static data.","title":"Use Sources and Seeds"},{"location":"blog/dbt-best-practices/#modularize-models","text":"Break transformations into small, reusable models.","title":"Modularize Models"},{"location":"blog/dbt-best-practices/#test-everything","text":"-- Example dbt test SELECT * FROM {{ ref ( 'my_model' ) }} WHERE id IS NULL","title":"Test Everything"},{"location":"blog/dbt-best-practices/#document-your-models","text":"Use dbt's built-in documentation features.","title":"Document Your Models"},{"location":"blog/dbt-best-practices/#running-on-cloud","text":"Use ECS along with EventBridge to run as an event driven transformation pipeline","title":"Running on cloud"},{"location":"blog/my-developer-setup/","text":"Developer setup (macOS) A compact list of tools and a minimal ~/.zshrc snippet used for local development. zsh \u2014 Modern, configurable shell with strong plugin support. zsh-autosuggestions / zsh-autocomplete \u2014 Inline suggestions or completion to speed typing. starship \u2014 Fast, informative prompt that shows Git/Python status and other context. Nerd font (e.g., CascadiaCode Nerd Font) \u2014 Provides icons and ligatures for prompts and editors. eza \u2014 Modern replacement for ls with color, git integration, and sensible defaults. Minimal ~/.zshrc snippet # environment and path . \" $HOME /.local/bin/env\" export PATH = \" $HOME /.local/bin: $PATH \" # starship prompt eval \" $( starship init zsh ) \" # zsh autosuggestions (or autocomplete) source $( brew --prefix ) /share/zsh-autosuggestions/zsh-autosuggestions.zsh # alias using eza alias ll = 'eza -l --reverse --time=modified --long' Tips: keep packages updated via Homebrew, install fonts with Homebrew Cask (or a font manager), and use the scripts/serve.sh helper to preview the site locally before publishing.","title":"My developer setup"},{"location":"blog/my-developer-setup/#developer-setup-macos","text":"A compact list of tools and a minimal ~/.zshrc snippet used for local development. zsh \u2014 Modern, configurable shell with strong plugin support. zsh-autosuggestions / zsh-autocomplete \u2014 Inline suggestions or completion to speed typing. starship \u2014 Fast, informative prompt that shows Git/Python status and other context. Nerd font (e.g., CascadiaCode Nerd Font) \u2014 Provides icons and ligatures for prompts and editors. eza \u2014 Modern replacement for ls with color, git integration, and sensible defaults.","title":"Developer setup (macOS)"},{"location":"blog/my-developer-setup/#minimal-zshrc-snippet","text":"# environment and path . \" $HOME /.local/bin/env\" export PATH = \" $HOME /.local/bin: $PATH \" # starship prompt eval \" $( starship init zsh ) \" # zsh autosuggestions (or autocomplete) source $( brew --prefix ) /share/zsh-autosuggestions/zsh-autosuggestions.zsh # alias using eza alias ll = 'eza -l --reverse --time=modified --long' Tips: keep packages updated via Homebrew, install fonts with Homebrew Cask (or a font manager), and use the scripts/serve.sh helper to preview the site locally before publishing.","title":"Minimal ~/.zshrc snippet"},{"location":"blog/python-data-cleaning/","text":"Python Data Cleaning Tricks Cleaning data is a crucial step in any analytics workflow. Here are some Python tips for effective data cleaning using pandas: Remove Nulls df = df . dropna () Standardize Text df [ 'city' ] = df [ 'city' ] . str . lower () . str . strip () Convert Data Types df [ 'date' ] = pd . to_datetime ( df [ 'date' ])","title":"Python Data Cleaning Tricks"},{"location":"blog/python-data-cleaning/#python-data-cleaning-tricks","text":"Cleaning data is a crucial step in any analytics workflow. Here are some Python tips for effective data cleaning using pandas:","title":"Python Data Cleaning Tricks"},{"location":"blog/python-data-cleaning/#remove-nulls","text":"df = df . dropna ()","title":"Remove Nulls"},{"location":"blog/python-data-cleaning/#standardize-text","text":"df [ 'city' ] = df [ 'city' ] . str . lower () . str . strip ()","title":"Standardize Text"},{"location":"blog/python-data-cleaning/#convert-data-types","text":"df [ 'date' ] = pd . to_datetime ( df [ 'date' ])","title":"Convert Data Types"},{"location":"blog/snowflake-tips/","text":"Snowflake Tips for Data Engineers Snowflake is a popular cloud data warehouse. Here are some tips for getting the most out of it: Use Zero-Copy Cloning Quickly create dev/test environments without duplicating data. Time Travel Restore data to a previous state using Snowflake\u2019s time travel feature. Query History Monitor and optimize queries using the QUERY_HISTORY view. Example: Cloning a Table CREATE TABLE my_table_clone CLONE my_table ;","title":"Snowflake Tips for Data Engineers"},{"location":"blog/snowflake-tips/#snowflake-tips-for-data-engineers","text":"Snowflake is a popular cloud data warehouse. Here are some tips for getting the most out of it:","title":"Snowflake Tips for Data Engineers"},{"location":"blog/snowflake-tips/#use-zero-copy-cloning","text":"Quickly create dev/test environments without duplicating data.","title":"Use Zero-Copy Cloning"},{"location":"blog/snowflake-tips/#time-travel","text":"Restore data to a previous state using Snowflake\u2019s time travel feature.","title":"Time Travel"},{"location":"blog/snowflake-tips/#query-history","text":"Monitor and optimize queries using the QUERY_HISTORY view.","title":"Query History"},{"location":"blog/snowflake-tips/#example-cloning-a-table","text":"CREATE TABLE my_table_clone CLONE my_table ;","title":"Example: Cloning a Table"},{"location":"blog/sql-tips/","text":"10 Essential SQL Tips for Data Analysts SQL is the backbone of data analytics. Here are 10 tips to write better, faster, and more reliable SQL queries. 1. Use CTEs for Readability WITH recent_orders AS ( SELECT * FROM orders WHERE order_date > '2025-01-01' ) SELECT * FROM recent_orders ; 2. Prefer INNER JOIN over WHERE for joins ... 3. Use EXPLAIN to Analyze Query Plans ...","title":"10 Essential SQL Tips for Data Analysts"},{"location":"blog/sql-tips/#10-essential-sql-tips-for-data-analysts","text":"SQL is the backbone of data analytics. Here are 10 tips to write better, faster, and more reliable SQL queries.","title":"10 Essential SQL Tips for Data Analysts"},{"location":"blog/sql-tips/#1-use-ctes-for-readability","text":"WITH recent_orders AS ( SELECT * FROM orders WHERE order_date > '2025-01-01' ) SELECT * FROM recent_orders ;","title":"1. Use CTEs for Readability"},{"location":"blog/sql-tips/#2-prefer-inner-join-over-where-for-joins","text":"...","title":"2. Prefer INNER JOIN over WHERE for joins"},{"location":"blog/sql-tips/#3-use-explain-to-analyze-query-plans","text":"...","title":"3. Use EXPLAIN to Analyze Query Plans"},{"location":"blog/syntax-highlighting-showcase/","text":"Syntax Highlighting Showcase This post demonstrates the full range of syntax highlighting for Python, SQL, and Bash code blocks using your current theme. Python Example # Python: Data cleaning and plotting import pandas as pd import numpy as np import matplotlib.pyplot as plt def clean_data ( df ): df = df . dropna () df [ 'value' ] = df [ 'value' ] . astype ( float ) return df [ df [ 'value' ] > 0 ] class DataPlotter : def __init__ ( self , df ): self . df = df def plot ( self ): plt . plot ( self . df [ 'date' ], self . df [ 'value' ], label = 'Value' ) plt . xlabel ( 'Date' ) plt . ylabel ( 'Value' ) plt . title ( 'Data Over Time' ) plt . legend () plt . show () if __name__ == '__main__' : df = pd . DataFrame ({ 'date' : pd . date_range ( '2023-01-01' , periods = 5 ), 'value' : [ 1 , 2 , np . nan , 4 , 5 ]}) df = clean_data ( df ) plotter = DataPlotter ( df ) plotter . plot () SQL Example -- SQL: Create, insert, and query CREATE TABLE users ( id INTEGER PRIMARY KEY , username TEXT NOT NULL , email TEXT UNIQUE , created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); INSERT INTO users ( username , email ) VALUES ( 'alice' , 'alice@example.com' ); INSERT INTO users ( username , email ) VALUES ( 'bob' , 'bob@example.com' ); SELECT id , username , email FROM users WHERE email LIKE '%@example.com%' ORDER BY created_at DESC ; Bash Example # Bash: Data backup script #!/bin/bash set -euo pipefail SRC_DIR = \"/home/user/data\" BACKUP_DIR = \"/mnt/backup\" DATE = $( date +%Y-%m-%d ) mkdir -p \" $BACKUP_DIR / $DATE \" cp -av \" $SRC_DIR \" /* \" $BACKUP_DIR / $DATE /\" echo \"Backup completed for $DATE \" Enjoy the colors!","title":"Syntax Highlighting Showcase"},{"location":"blog/syntax-highlighting-showcase/#syntax-highlighting-showcase","text":"This post demonstrates the full range of syntax highlighting for Python, SQL, and Bash code blocks using your current theme.","title":"Syntax Highlighting Showcase"},{"location":"blog/syntax-highlighting-showcase/#python-example","text":"# Python: Data cleaning and plotting import pandas as pd import numpy as np import matplotlib.pyplot as plt def clean_data ( df ): df = df . dropna () df [ 'value' ] = df [ 'value' ] . astype ( float ) return df [ df [ 'value' ] > 0 ] class DataPlotter : def __init__ ( self , df ): self . df = df def plot ( self ): plt . plot ( self . df [ 'date' ], self . df [ 'value' ], label = 'Value' ) plt . xlabel ( 'Date' ) plt . ylabel ( 'Value' ) plt . title ( 'Data Over Time' ) plt . legend () plt . show () if __name__ == '__main__' : df = pd . DataFrame ({ 'date' : pd . date_range ( '2023-01-01' , periods = 5 ), 'value' : [ 1 , 2 , np . nan , 4 , 5 ]}) df = clean_data ( df ) plotter = DataPlotter ( df ) plotter . plot ()","title":"Python Example"},{"location":"blog/syntax-highlighting-showcase/#sql-example","text":"-- SQL: Create, insert, and query CREATE TABLE users ( id INTEGER PRIMARY KEY , username TEXT NOT NULL , email TEXT UNIQUE , created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); INSERT INTO users ( username , email ) VALUES ( 'alice' , 'alice@example.com' ); INSERT INTO users ( username , email ) VALUES ( 'bob' , 'bob@example.com' ); SELECT id , username , email FROM users WHERE email LIKE '%@example.com%' ORDER BY created_at DESC ;","title":"SQL Example"},{"location":"blog/syntax-highlighting-showcase/#bash-example","text":"# Bash: Data backup script #!/bin/bash set -euo pipefail SRC_DIR = \"/home/user/data\" BACKUP_DIR = \"/mnt/backup\" DATE = $( date +%Y-%m-%d ) mkdir -p \" $BACKUP_DIR / $DATE \" cp -av \" $SRC_DIR \" /* \" $BACKUP_DIR / $DATE /\" echo \"Backup completed for $DATE \" Enjoy the colors!","title":"Bash Example"},{"location":"projects/project1/","text":"Project 1 GitHub Repo Write a blog or detailed explanation about Project 1 here.","title":"Project 1"},{"location":"projects/project1/#project-1","text":"GitHub Repo Write a blog or detailed explanation about Project 1 here.","title":"Project 1"},{"location":"projects/project2/","text":"Project 2 GitHub Repo Write a blog or detailed explanation about Project 2 here.","title":"Project 2"},{"location":"projects/project2/#project-2","text":"GitHub Repo Write a blog or detailed explanation about Project 2 here.","title":"Project 2"}]}